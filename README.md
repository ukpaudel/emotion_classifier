# Summary

**emotion_classifier** is a modular, production-ready Python framework for speech emotion classification. It leverages powerful self-supervised speech encoders (like Wav2Vec2 or HuBERT) with an attention-based classifier on top, allowing easy transfer learning and fine-tuning for emotion recognition tasks.

It is designed to be configuration-driven, making training, evaluation, and experimentation easy and reproducible.

---

## ðŸš€ Features

- ðŸ”Œ Plug-and-play support for self-supervised encoders (tested with HuBERT, Wav2Vec2)
- â„ï¸ Encoder freezing or selective fine-tuning of the last *N* layers (`unfreeze_last_n_layers`)
- ðŸ§© Dynamic masking support for variable-length audio
- ðŸ—‚ï¸ Extensible to new datasets â€” just add a new dataset loader like `ravdess_dataset.py`
- ðŸ’¾ Pretrained weight loading with `pretrained_weights.enabled: true`
- ðŸ“ TensorBoard integration and logging for visualization
- ðŸ”„ Checkpoint-based resume functionality
- ðŸŽ¯ Supports single-file inference and real-time emotion detection with ASR
- ðŸ› Automatic saving of misclassified audio samples for inspection (Need to update)
- ðŸ› ï¸ Configurable entirely through `configs/config.yml`

---
## Install Dataset
RAVDESS: https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio
Download and extract it into data/ravdess/data/actor_1, actor_2, etc. 
## Folder Structure

```text
emotion_classifier/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ config.yml
â”‚   â”œâ”€â”€ models_runs.yml
â”‚   â””â”€â”€ inference_config.yml
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ ravdess_dataset.py
â”‚   â”œâ”€â”€ cremad_dataset.py
â”‚   â”œâ”€â”€ dataloader.py
â”‚   â”œâ”€â”€ collate.py
â”‚   â”œâ”€â”€ ravdess/
â”‚   â”‚   â””â”€â”€ data/
â”‚   â””â”€â”€ CREMA-D/
â”‚       â””â”€â”€ data/
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ attention_classifier.py
â”‚   â””â”€â”€ emotion_model.py
â”‚
â”œâ”€â”€ runs/
â”‚   â”œâ”€â”€ emotion_classifier_v#/
â”‚   â”‚   â””â”€â”€ run_label/  # stored based on config file logging.run_label
â”‚   â”‚       â”œâ”€â”€ checkpoint.pt  # trained weights
â”‚   â”‚       â””â”€â”€ events.out.tfevents  # tensorboard logs
â”‚   â”œâ”€â”€ example_comparisons.png
â”‚   â”œâ”€â”€ encoder_loader.py
â”‚   â”œâ”€â”€ run_tracker.py
â”‚   â”œâ”€â”€ model_utils.py
â”‚   â””â”€â”€ tensorboard_plot_utils.py
â”‚
â”œâ”€â”€ train/
â”‚   â””â”€â”€ trainer.py
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ encoder_loader.py
â”‚   â”œâ”€â”€ run_tracker.py
â”‚   â”œâ”€â”€ model_utils.py
â”‚   â””â”€â”€ tensorboard_plot_utils.py
â”‚
â”œâ”€â”€ inference/
â”‚   â””â”€â”€ realtime_inference_withASR.py
â”‚
â”œâ”€â”€ run_experiment.py
â”œâ”€â”€ setup.py
â”œâ”€â”€ README.md
â””â”€â”€ ...
```


## Architecture
```
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚         Waveform Input        â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  Self-Supervised Encoder      â”‚
               â”‚   (Wav2Vec2, HuBERT, WavLM)   â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚    Variable-Length Features   â”‚
               â”‚  (sequence of 768-dim tokens) â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                       (with optional mask)
                              â”‚
                              â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚     Attention Pooling         â”‚
               â”‚   (weighted average pooling)  â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  Feedforward Classifier (MLP) â”‚
               â”‚    (with Dropout + GELU)      â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚       Emotion Logits          â”‚
               â”‚      (one of 8 classes)       â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


```
## âš™ï¸ Installation

```bash
git clone https://github.com/ukpaudel/emotion_classifier.git
cd emotion_classifier
pip install -e .

```

## Usage

### Training

```
>cd emotion_classifier
>python .\run_experiment.py 

This will read parameters from configs/config.yml, including:
encoder type
dataset path
hyperparameters
output directories
resume or pretrained settings

```
### Visualize Comparisons Between Different Experiments
To Visualize Tensboard Weights go to bash and run ```  > tensorboard --logdir=runs and open http://localhost:6006/ ```

### Inference 
For realtime
To run go to the root folder ```cd emotion_classifier\emotion_classifier```  
``` > python .\inference\realtime_inference_withASR.py```

for single audio
``` > python .\inference\realtime_inference_withASR.py```

## Results.
see comparison between different trainings in 
```runs\example_comparisons.png``` file. 
Verdict: mixed results. Saw as high as closer to 80% classification accuracy with RAVDAV dataset. Adding CREMA-D dataset made the classification accuracy go hover around 70%. Unfreezing the encoder layers gave inconsistent results, for RAVDAV dataset, it increased the efficiency significantly, though I am concerned that the validation dataset was too small and the model was overfitting.
Wav2Vec2 had poorer performance. HuBert results in increase classification accuracy with model converging with fewer epoches.
With frozen encoder layers and only RAVDEV audio dataset. ![image](https://github.com/user-attachments/assets/a06e94a5-a9e8-4f12-ae0d-a7df148ec078)
Example of Tensorboard: <img width="1000" alt="example_comparisons" src="https://github.com/user-attachments/assets/9764f031-abff-4670-a537-e574633b5f28" />

## Author
Uttam Paudel
paudeluttam@gmail.com

## License

[MIT](https://choosealicense.com/licenses/mit/)
